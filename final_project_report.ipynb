{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6b023",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117997e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We implement distributed training based on the Needle framework in our final project. In distributed training, the workload to train a model is split up and shared among multiple devices like GPUs, called nodes. These nodes work in parallel to speed up model training. The two main types of distributed training are data parallelism and model parallelism. In short, data parallelism divides the training data into partitions; model parallelism segments the model into different parts that can run concurrently in different nodes [1]. This project implmements the data parallism apporach. We'll elaborate a bit more about data parallelism in the following sections.\n",
    "\n",
    "In data parallelism, the training data is divided into partitions, where the number of partitions is equal to the total number of available nodes. The partitions are assigned to the available nodes.\n",
    "The model is copied in each of these nodes and each nodes operates on its own subset of the partition. Each node calculates the gradients of the model parameters independently. The calculated gradients of the nodes are aggragated to obtain the average gradients. Finally, each node updates the model parameters using the average gradients. \n",
    "\n",
    "Here we also give a brief explanation of the mathematical theory of data parallelism. Let $w$ be the parameters of the model; $\\frac{\\delta{L}}{\\delta{w}}$ is the original gradients of the batch of size $n$; $l_i$ is the loss for data point $i$ and $k$ is the number of nodes. Then we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{\\delta[\\frac{1}{n}\\sum_{i=1}^{n}l_i]}{\\delta{w}} \\\\\n",
    "                              =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\delta{l_i}}{\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\frac{1}{m_1}\\sum_{i=1}^{m_1}l_i}{\\delta{w}} \n",
    "                               +\\frac{m_2}{n}\\frac{\\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m2}l_i}{\\delta{w}}\n",
    "                               + \\dots\n",
    "                               + \\frac{m_k}{n}\\frac{\\frac{1}{m_k}\\sum_{i=m_{k-1}+1}^{m_{k-1}+m_{k}}l_i} {\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{m_2}{n}\\frac{\\delta{l_2}}{\\delta{w}}\n",
    "                              +\\dots+\\frac{m_k}{n}\\frac{\\delta{l_k}}{\\delta{w}}\n",
    "$$\n",
    "where $m_k$ is the number of data points assigned to node $k$, and \n",
    "$$\n",
    "m_1+m_2+\\dots+m_{k}=n\n",
    "$$\n",
    "If $m_1=m_2=\\dots=m_k=\\frac{n}{k}$, we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{1}{k}[\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{\\delta{l_2}}{\\delta{w}}+\\dots+\\frac{\\delta{l_k}}{\\delta{w}}]\n",
    "$$\n",
    "where $\\frac{\\delta{l_k}}{\\delta{w}}$ means the gradients calculated by node $k$ based on the data points $\\{m_{k-1}+1,m_{k-1}+2,\\dots,m_{k-1}+m_k\\}$.\n",
    "According to the above equation, we could know that the average gradients of all the nodes are equal to the original gradients [2]. \n",
    "\n",
    "We have two implementations of distributed trainig, which are based on different communication frameworks, i.e., mpi4py [3] and nccl [4]. The source code of the project can be found here: [TODO]. We intorduce these two implementations in section 2 and section 3 respectively.\n",
    "\n",
    "To test distributed training, you need to run this notebook with multiple GPUs. Run  `nvidia-smi` to check how many GPUs are available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c728c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  9 11:39:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   24C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    56W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0828c5",
   "metadata": {},
   "source": [
    "Clone the cod and install necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b725ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "\n",
    "# !git clone https://github.com/jzh18/hw4.git\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install pybind11\n",
    "# !pip3 install mpi4py\n",
    "# %cd /content/drive/MyDrive/hw4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62159f8",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9df1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-10-batches-py/\n",
      "cifar-10-batches-py/data_batch_4\n",
      "cifar-10-batches-py/readme.html\n",
      "cifar-10-batches-py/test_batch\n",
      "cifar-10-batches-py/data_batch_3\n",
      "cifar-10-batches-py/batches.meta\n",
      "cifar-10-batches-py/data_batch_2\n",
      "cifar-10-batches-py/data_batch_5\n",
      "cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46d106",
   "metadata": {},
   "source": [
    "Comiple the souce code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6e96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/x_huzha/.conda/envs/dlsys/lib/python3.8/site-packages/pybind11/include (found version \"2.10.2\")\n",
      "CMake Warning (dev) at CMakeLists.txt:56 (find_package):\n",
      "  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.\n",
      "  Run \"cmake --help-policy CMP0074\" for policy details.  Use the cmake_policy\n",
      "  command to set the policy and suppress this warning.\n",
      "\n",
      "  Environment variable CUDA_ROOT is set to:\n",
      "\n",
      "    /software/sse/manual/CUDA/11.3.1_465.19.01\n",
      "\n",
      "  For compatibility, CMake is ignoring the variable.\n",
      "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Jan  9 14:26:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    54W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.0 8.0 8.0\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /proj/berzelius-2022-9-video/users/x_huzha/hw4/build\n",
      "make[1]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[2]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[1]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702cd1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Distributed training with mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee2755",
   "metadata": {},
   "source": [
    "In this section, we introduce the usage and implementation details of distributed trainig based on mpi4py framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2524537",
   "metadata": {},
   "source": [
    "## 2.1 Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d83bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we demostrate how to use distributed training.\n",
    "\n",
    "In this project, we tried to create the process similar to what horovod provides. The training process will take place in different process at the same time, and each process would communicate with each other through Message Passing Interface (MPI) protocol.\n",
    "\n",
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af90e8a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the file `train_resnet.py`, we use distributed training to train a ResNet9 model. Let's walk through the code in `train_resnet.py` briefly to show how to use distributed training.\n",
    "\n",
    "Firstly, import the packages we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbffe10",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "```\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "from models import ResNet9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374bd50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After importing what we need from the basic needle framework, we now can import the ddp (distributed data parallel) from apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc56e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "```\n",
    "import apps.ddp as ddp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68c6ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we are going to initialize everything we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0565eb",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "```\n",
    "# this function initialize the ddp functionality\n",
    "# and return a desired cuda device\n",
    "rank, device = ddp.init()\n",
    "\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "\n",
    "#  this function do the partition for dataset and\n",
    "#  returns a dataloader and batch_size for the current process\n",
    "train_dataloader, bsz = ddp.partition_dataset(\n",
    "    dataset=dataset, batch_size=128, device=device, dtype='float32')\n",
    "\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "#  Before training, we must broadcast the parameters to different process\n",
    "ddp.broadcast_parameters(model)\n",
    "\n",
    "model.train()\n",
    "opt = ndl.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "#  After defining the optimizer, we need to call this to\n",
    "#  make the optimizer work for distributed class\n",
    "opt  = ddp.DistributedOptimizer(opt)\n",
    "\n",
    "loss_fn = ndl.nn.SoftmaxLoss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbec0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After the initialization, the training step is very simple. Here we can see that the training process is similar to what we normally do in needle framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dac467",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "```\n",
    "n_epochs = 1\n",
    "for i in range(n_epochs):\n",
    "    if rank == 0:\n",
    "        print(f'epoch: {i+1}/{n_epochs}')\n",
    "    for batch in train_dataloader:\n",
    "        opt.reset_grad()\n",
    "        X, y = batch\n",
    "        out = model(X)\n",
    "        correct = np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cce9d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using pytorch to find how many gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37857d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_huzha/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa79f1d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have 3 GPUs here. Now, let's train the ResNet model using distributed training with the 3 GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f06980",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train_resnet.py\", line 17, in <module>\n",
      "    dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 258, in __init__\n",
      "    dict = self._unpickle(os.path.join(base_folder, f))\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 267, in _unpickle\n",
      "    with open(file, 'rb') as fo:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/cifar-10-batches-py/data_batch_1'\n",
      "Use cuda: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"train_resnet.py\", line 17, in <module>\n",
      "    dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 258, in __init__\n",
      "    dict = self._unpickle(os.path.join(base_folder, f))\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 267, in _unpickle\n",
      "    with open(file, 'rb') as fo:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/cifar-10-batches-py/data_batch_1'\n",
      "Use cuda: 2\n",
      "Traceback (most recent call last):\n",
      "  File \"train_resnet.py\", line 17, in <module>\n",
      "    dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 258, in __init__\n",
      "    dict = self._unpickle(os.path.join(base_folder, f))\n",
      "  File \"/proj/berzelius-2022-9-video/users/x_huzha/hw4/./python/needle/data.py\", line 267, in _unpickle\n",
      "    with open(file, 'rb') as fo:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/cifar-10-batches-py/data_batch_1'\n",
      "--------------------------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpiexec detected that one or more processes exited with non-zero status, thus causing\n",
      "the job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[5030,1],2]\n",
      "  Exit code:    1\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 3 python train_resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45daef8",
   "metadata": {},
   "source": [
    "Training ResNet for 1 epoch with 3 GPUs takes about 54s. Let's train it with one GPU and see how long it would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ee6ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 0\n",
      "partitioned dataset length: 50000\n",
      "orignal dataset length: 50000\n",
      "epoch: 1/1\n",
      "acc: 0.1015625; avg_loss: 3.54433012008667\n",
      "acc: 0.34375; avg_loss: 1.7215627431869507\n",
      "acc: 0.4453125; avg_loss: 1.5035836696624756\n",
      "acc: 0.40625; avg_loss: 1.7049064636230469\n",
      "Training Time: 85.58778405189514\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 1 python train_resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce57fe",
   "metadata": {},
   "source": [
    "As you can see, training resnet for 1 epoch with 1 GPU takes about 85s. By distributed trainig, we reduce the training time for 37% ((85-54)/85)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e279b",
   "metadata": {},
   "source": [
    "## 2.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5bba1",
   "metadata": {},
   "source": [
    "In this section, we show the implementation details of distributed training in our project. Most of the code relevant to distributed training are located in the file `apps/ddp.py`.\n",
    "\n",
    "\n",
    "The `DataPartitioner` in the file divides a dataset into multiple partitions with the size specified by users. The code are shown as below:\n",
    "```\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The `broadcast_parameters` function broadcasts the model parameters to all the nodes. The following shows the code of `broadcast_parameters` function:\n",
    "```\n",
    "def broadcast_parameters(model, root_rank=0):\n",
    "    for p in model.parameters():\n",
    "        p_data = p.numpy()\n",
    "        p_data = comm.bcast(p_data, root=0)\n",
    "        p.data = ndl.Tensor(p_data, device=device, dtype=p.dtype)\n",
    "```\n",
    "\n",
    "The `DistributedOptimizer` class uses the all-reduce functionality to aggrate the gradients calculated by each nodes and calculate the mean of these gradients. The model of each node update the model parameters based on the mean gradients. The code of `DistributedOptimizer` are shown below.\n",
    "```\n",
    "class DistributedOptimizer(ndl.optim.Optimizer):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__(opt.params)\n",
    "        self.opt = opt\n",
    "\n",
    "    def step(self):\n",
    "        self.average_gradients()\n",
    "        self.opt.step()\n",
    "\n",
    "    def average_gradients(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            sendbuf = np.ascontiguousarray(p.grad.numpy())\n",
    "            recvbuf = np.empty_like(sendbuf, dtype=p.dtype)\n",
    "            comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)\n",
    "            recvbuf = recvbuf / world_size\n",
    "            p.grad.data = ndl.Tensor(recvbuf, device=device, dtype=p.grad.dtype)\n",
    "```\n",
    "\n",
    "\n",
    "In order to select the GPU we want to run the training workloads, we add a function named `SetDevice(int32_t device_id)` in `src/ndarray_backend_cuda.cu`. Users need to specify the device_id when invoking `needle.cuda(device_id)`. For example,\n",
    "`needle.cuda(1)` return a device which represents GPU 1. The code of `SetDevice(int32_t device_id)` are shown below.\n",
    "```\n",
    "void SetDevice(int32_t device_id) {\n",
    "  cudaSetDevice(device_id);  \n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633f9b9",
   "metadata": {},
   "source": [
    "# 3. Distributed training with NCCL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b3a8d",
   "metadata": {},
   "source": [
    "In section two, we implemented distributed trainging using MPI communication API. This type of communication is very inconvenient, we need to turn the data into numpy, and use CPU to communicate. It doesn't take advantage of multiple GPUs. Therefore it is essential to use the NVIDIA Collective Communication Library(NCCL), which is developed by NVIDIA official."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fec335",
   "metadata": {},
   "source": [
    "## 3.1 Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be955c",
   "metadata": {},
   "source": [
    "The usage are quite similar..// todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8da9dd",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Implementation\n",
    "\n",
    "\n",
    "To enable direct communication between GPUs in NCCL, we should crreate a communicator first. In terms of concrete implementation, first we need to call the `ncclGetUniqueId()` function, it will return an ID, which will be used by all processees and threads to synchronize and understand they are part of the same communicator. Then we can use `ncclCommInitRank()` to create the communicator objects. The key issue is that we need to broadcast ID to all participating threads and processes using any CPU communication system. In the original MPI with CUDA program, we can call the CUDA-based MPI API to finish the broadcast. But in our project, we call CUDA program via Python, MPI is also based on Python. As a result, we can't use the CUDA-based MPI API but we can use the Python-based. \n",
    "\n",
    "Our solutions are as follows:\n",
    "\n",
    "1. Python program calls CUDA API, CUDA program gets the ID and returns it to Python.\n",
    "2. Python program calls Python-based MPI API to broadcast the ID.\n",
    "3. All processees and threads get the same ID, calls CUDA API to establish a connection.\n",
    "\n",
    "\n",
    "The relevant codes arre as follows:\n",
    "\n",
    "Python code:\n",
    "```\n",
    "def init():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    rank = comm.Get_rank() # call MPI API to get world_size and rank\n",
    "    device = ndl.cuda(rank) # choose different GPUs\n",
    "    print(f'Use cuda: {rank}')\n",
    "\n",
    "    if rank==0:\n",
    "        vec = device.get_id() # get ID\n",
    "    else:\n",
    "        vec = None\n",
    "    vec = comm.bcast(vec, root=0) # broadcast ID\n",
    "\n",
    "    device.init_nccl(vec,rank,size) # establish a connection\n",
    "    return rank, size, device\n",
    "```\n",
    "\n",
    "CUDA code:\n",
    "```\n",
    "struct CudaCommAndStream{\n",
    "    int nRanks,localRank,myRank;\n",
    "    ncclUniqueId id;\n",
    "    ncclComm_t comm;\n",
    "    cudaStream_t s;\n",
    "}mess;\n",
    "void SetDevice(int id) # set different device\n",
    "{\n",
    "    mess.localRank=id;\n",
    "    cudaSetDevice(id);\n",
    "}\n",
    "std::vector<uint8_t> GetId()\n",
    "{\n",
    "    ncclGetUniqueId(&mess.id); # get id \n",
    "    auto vec = std::vector<uint8_t>(reinterpret_cast<uint8_t*>(&mess.id),reinterpret_cast<uint8_t*>(&mess.id) + NCCL_UNIQUE_ID_BYTES); # put id into vector\n",
    "    return vec;\n",
    "}\n",
    "\n",
    "void InitNccl(std::vector<uint8_t> vec,int rank,int size) \n",
    "{\n",
    "    mess.nRanks = size;\n",
    "    mess.myRank = rank;\n",
    "    std::memcpy(&mess.id, vec.data(), vec.size()); # change vector to id\n",
    "    ncclCommInitRank(&mess.comm, mess.nRanks, mess.id, mess.myRank); # establish a connection\n",
    "    cudaStreamCreate(&mess.s);\n",
    "}\n",
    "PYBIND11_MODULE(ndarray_backend_cuda, m) {\n",
    "    ...\n",
    "    m.def(\"set_device\", SetDevice);\n",
    "    m.def(\"get_id\", GetId);\n",
    "    m.def(\"init_nccl\", InitNccl);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0130fa",
   "metadata": {},
   "source": [
    "# X. Decentralized ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfdaeb4",
   "metadata": {},
   "source": [
    "## X.1 Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8ae33",
   "metadata": {},
   "source": [
    "Our decentralized ML includes two main components: (1) The Needle framework, which is a PyTorch-like ML framework, which supports both distributed training and decentralized training. (2) The go-needle client, which is a client for the decentralized network.\n",
    "\n",
    "User A uses the Needle framework to write the code of their computing tasks and submit the task to the decentralized network. \n",
    "\n",
    "User B uses the go-needle client to join the decentralized network to provide their computing resources. The tasks submitted by User A will be executed on User B's computing resources. And user B will get the reward once it finishes the task and the result is verified by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042dfdcb",
   "metadata": {},
   "source": [
    "### X.1.1 Needle Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c8233",
   "metadata": {},
   "source": [
    "The Needle framework converts users' code into a computational graph. The operators of the graph and the data are separated.\n",
    "The data, which might be very large, is stored in the decentralized storage.\n",
    "The computational graph encapsulates the graph into a transaction.\n",
    "The transaction is submitted to the decentralized network.\n",
    "\n",
    "An example of a transaction:\n",
    "```json\n",
    "{\n",
    "    \"submitter_address\": \"0x123abcd\", // who submits the transaction\n",
    "    \"data\":{                          // the data of the computational graph\n",
    "        \"weight1\": \"0x21321\", // hash value of the data in the decentralized storage\n",
    "        \"weight2\": \"0xabcd2\",\n",
    "    },\n",
    "    \"ops\":{          // the operators of the computational graph\n",
    "        \"op1\":{\n",
    "            \"input1\": \"weight1\",\n",
    "            \"input2\": \"weight2\",\n",
    "            \"op\": \"matmul\",\n",
    "            \"output1\": \"output1\"\n",
    "        }\n",
    "    },\n",
    "    \"gas\": 1000  // how much gas the submitter is willing to pay for the transaction\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ea9d9",
   "metadata": {},
   "source": [
    "### X.1.2 Go-needle Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b99ee2",
   "metadata": {},
   "source": [
    "In the decentralized network, there are two types of nodes: the miner and the verifier. The miner is responsible for executing the transaction, i.e., perform the real computation. The verifier is responsible for verifying the result of the computation.\n",
    "Once a miner finishes the computation, it will pack the result into a block and submit the block to the network. The verifier will verify the result and get the reward if the result is correct.\n",
    "The network does not require how to result is computed, it only cares about whether the result is correct.\n",
    "So miners could use any methods to compute the result, e.g. using GPU(s), TPU(s), etc.\n",
    "\n",
    "An example of a block:\n",
    "```json\n",
    "{\n",
    "    \"miner\": \"0x123abcd\", // the address of the miner\n",
    "    \"transactions\":[ \n",
    "        {\n",
    "            \"tx_hash\": \"0x1234\", // the hash value of the transaction\n",
    "            \"resutls\": \"0xabcd\", // the hash value of the result, whch is stored in the decentralized storage\n",
    "        }\n",
    "    ],\n",
    "    \"parentHash\": \"0x1234\", // the hash value of the parent block\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27510b",
   "metadata": {},
   "source": [
    "### X.1.3 Verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833a55ca",
   "metadata": {},
   "source": [
    "Algorithmic way. Given a computational graph, use certain algorithms to verify the result. For example, if the computational graph is a sample matrix multiplication, we can use the Freivalds' algorithm to verify the result. The verification process should be much easier than the computation process.\n",
    "1. A common matrix multiplication is runs in O(n^3) time complexity.\n",
    "2. Freivalds' algorithm can verify the result in O(n^2) time complexity with a small probability of error.\n",
    "3. We can use multiple verifiers to verify the result to reduce the probability of error further.\n",
    "Problems: it might be difficult or even impossible to verify the result of more complex computation tasks.\n",
    "\n",
    "Challenging: We can randomly issue some challenging tasks to a miner, which we know the result in advance. If the miner can solve the challenging task, we can trust the miner to solve the real task.\n",
    "Problems: A malicious node might recognize the challenging task and solves the challenging task honestly but cheats on the real task.\n",
    "\n",
    "Vote: A task can be computed by multiple miners. The result is considered correct if the majority of the miners have the same result. \n",
    "Problems: Waste of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939d21c",
   "metadata": {},
   "source": [
    "## X.2 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdc7a4",
   "metadata": {},
   "source": [
    "### X.2.1 Setup the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fb1bb",
   "metadata": {},
   "source": [
    "Create an account and start a Miner on your local machine:\n",
    "```\n",
    "docker pull hfzhang6/pouw-geth-client:miner-mac-m1\n",
    "# Use the account address to start the miner\n",
    "docker run -it --name account-generator --entrypoint=\"\"  hfzhang6/pouw-geth-client:miner-mac-m1 geth account new\n",
    "```\n",
    "This will output something like:\n",
    "```\n",
    "Your new account is locked with a password. Please give a password. Do not forget this password.\n",
    "Password:\n",
    "Repeat password:\n",
    "\n",
    "Your new key was generated\n",
    "\n",
    "Public address of the key:   0x2ed1D3e06f268Dd207dab9f3F452252348d0eb57\n",
    "Path of the secret key file: /root/.ethereum/keystore/UTC--2024-12-14T08-01-18.629899881Z--2ed1d3e06f268dd207dab9f3f452252348d0eb57\n",
    "\n",
    "- You can share your public address with anyone. Others need it to interact with you.\n",
    "- You must NEVER share the secret key with anyone! The key controls access to your funds!\n",
    "- You must BACKUP your key file! Without the key, it's impossible to access account funds!\n",
    "- You must REMEMBER your password! Without the password, it's impossible to decrypt the key!\n",
    "```\n",
    "The public address is your account address. Use this address to start the miner:\n",
    "```\n",
    "docker run -it --name my-miner -p 8545:8545 -p 30303:30303  -e MINER_ADDRESS=0x2ed1D3e06f268Dd207dab9f3F452252348d0eb57 hfzhang6/pouw-geth-client:miner-mac-m1\n",
    "```\n",
    "The follow code use our `Needle` framework to submit a task to the decentralized network.\n",
    "The task is simple, just calculate the result of a matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b87d7c",
   "metadata": {},
   "source": [
    "### X.2.2 Setup the Needle framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "876449ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NEEDLE_ACCOUNT_ADDRESS=0xAa6aC6b973f88FDb92f742e97210F65492c53DF5\n",
      "env: NEEDLE_ACCOUNT_PRIVATEKEY=0x93c55b87602cb466ae2789d1e391c25208a4a356596d406394bd70914d243ee9\n",
      "env: NEEDLE_NODE_ADDRESS=http://13.50.117.77:8545\n"
     ]
    }
   ],
   "source": [
    "# the address of the account\n",
    "%env NEEDLE_ACCOUNT_ADDRESS=0xAa6aC6b973f88FDb92f742e97210F65492c53DF5\n",
    "#USE_YOUR_OWN_KEY # the private key of the account\n",
    "%env NEEDLE_ACCOUNT_PRIVATEKEY=REPLACE_WITH_YOUR_PRIVATE_KEY\n",
    " # the address of a node in the decentralized network\n",
    "%env NEEDLE_NODE_ADDRESS=http://13.50.117.77:8545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1cf21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000000000000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the balance of the account\n",
    "from web3 import Web3\n",
    "w3 = Web3(Web3.HTTPProvider(os.getenv('NEEDLE_NODE_ADDRESS')))\n",
    "w3.eth.get_balance(os.getenv('NEEDLE_ACCOUNT_ADDRESS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59245e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x93c55b87602cb466ae2789d1e391c25208a4a356596d406394bd70914d243ee9\n",
      "c2f335e57627ceceaa5244b8afaa9ad6e97fa0d943ea9cc09f33b95bd200176f\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from needle import backend_ndarray as nd\n",
    "\n",
    "A=ndl.Tensor(nd.array([[1,2],[3,4]]),requires_grad=False)\n",
    "B=ndl.Tensor(nd.array([[1,2,3,4],[3,4,5,6]]), requires_grad=False)\n",
    "tx=ndl.ops.decentralized_matmul(A,B) # submit the task\n",
    "print(tx.tx_hash.hex()) # print the transaction hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07b172",
   "metadata": {},
   "source": [
    "The above task will be executed on a node in the network and the results can be retrieved querying the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b337e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10 13 16]\n",
      " [15 22 29 36]]\n"
     ]
    }
   ],
   "source": [
    "print(tx.get_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1840de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttributeDict({'difficulty': 131072,\n",
       " 'extraData': HexBytes('0xd883010b06846765746888676f312e31392e34856c696e7578'),\n",
       " 'gasLimit': 12011717,\n",
       " 'gasUsed': 22564,\n",
       " 'hash': HexBytes('0x8e66d154329b1e4a27568b1e0b9949b1933e9eecb75ce12ca8a51f819d6352ae'),\n",
       " 'logsBloom': HexBytes('0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'),\n",
       " 'miner': '0x2ed1D3e06f268Dd207dab9f3F452252348d0eb57',\n",
       " 'mixHash': HexBytes('0x0000000000000000000000000000000000000000000000000000000000000000'),\n",
       " 'nonce': HexBytes('0x070a0d100f161d24'),\n",
       " 'number': 1,\n",
       " 'parentHash': HexBytes('0x988dc110b264279abf530a192784d6e74ed1332ce24ae2c9e044b2ffb7bbeb72'),\n",
       " 'receiptsRoot': HexBytes('0x3568fbb2b2c2e2dc2120592677b5d7b3c80f05bcb093c1628a6bc691734f0d3b'),\n",
       " 'sha3Uncles': HexBytes('0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347'),\n",
       " 'size': 666,\n",
       " 'stateRoot': HexBytes('0x15e75d2d50c2d2bd9580409740b9a5949823463fe063561818957d2fbf689b9a'),\n",
       " 'timestamp': 1734170001,\n",
       " 'totalDifficulty': 131073,\n",
       " 'transactions': [HexBytes('0xc2f335e57627ceceaa5244b8afaa9ad6e97fa0d943ea9cc09f33b95bd200176f')],\n",
       " 'transactionsRoot': HexBytes('0xbe1dbf9d050f028acfdee6c4f43864eb2c8968a37343475d909be178acff5b53'),\n",
       " 'uncles': []})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the block that contains the transaction\n",
    "num_block=w3.eth.get_block_number()\n",
    "target_block=None\n",
    "for i in range(num_block):\n",
    "    block=w3.eth.get_block(i)\n",
    "    if tx.tx_hash in block['transactions']:\n",
    "        display(block)\n",
    "        target_block=block\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "092edc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miner balance:  2000045128000000000\n",
      "Submitter balance:  99999954872000000000\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the miner\n",
    "miner_address=target_block['miner']\n",
    "miner_balance=w3.eth.get_balance(miner_address)\n",
    "print(\"Miner balance: \",miner_balance)\n",
    "\n",
    "# Check the balance of the submitter account\n",
    "account_balance=w3.eth.get_balance(os.getenv('NEEDLE_ACCOUNT_ADDRESS'))\n",
    "print(\"Submitter balance: \",account_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128cb09",
   "metadata": {},
   "source": [
    "## X.2 Other Considerations\n",
    "1. Efficiency is a big problem in such a decentralized network. For the computational task with large size of parameters, a lot of time are needed to transfer the data between users, miners and the decentralized storage.\n",
    "2. Privacy is another problem. The data of the computational graph might be sensitive and should be protected.\n",
    "3. Centralization is also a problem. Complex tasks that require a lot of computing resources might can only be computed by a few miners, which is not decentralized.\n",
    "4. Reward mechanism. The reward should be based on the computational complexity, such as FLOPs (floating-point operations) of the computation. The real time a miner spends on the computation should also be considered. A shorter time should get a higher reward. \n",
    "5. Economic consideration. The amount of the reward keeps increasing, which might lead to the inflation of the network, which decentivizes the miners to join the network and leads to the failure of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c27e3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28133c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df901d6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[1] Distributed training. https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training\n",
    "\n",
    "[2] Data Parallelism VS Model Parallelism in Distributed Deep Learning Training. https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\n",
    "\n",
    "[3] Horovod Framework: https://horovod.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
